# Keyless Scraper Protocol

> **Address:** 1.3.11  
> **Type:** Protocol Module  
> **Status:** ✅ Production Ready  
> **Version:** 1.0.0

---

## Overview

The Keyless Scraper Protocol provides web scraping capabilities for publicly accessible content without requiring API keys or authentication. It includes features for ethical scraping including robots.txt compliance, rate limiting, and user-agent management.

```
┌──────────────────┐                    ┌──────────────────┐
│     Scraper      │                    │   Target Site    │
└────────┬─────────┘                    └────────┬─────────┘
         │                                       │
         │  1. GET /robots.txt                   │
         │──────────────────────────────────────▶│
         │◀─────────────────────────────────────│
         │     (Parse rules)                     │
         │                                       │
         │  2. GET /page (with User-Agent)       │
         │     (Wait for rate limit)             │
         │──────────────────────────────────────▶│
         │                                       │
         │  3. 200 OK + HTML                     │
         │◀──────────────────────────────────────│
         │     (Cache response)                  │
         │                                       │
         ▼                                       ▼
```

---

## ⚠️ Important Notice

This protocol is intended for **legitimate purposes only**:

✅ Public data that doesn't require login  
✅ Research and academic purposes  
✅ Personal use and archival  
✅ Data that website owners intend to be accessible  

**Always:**
- Respect robots.txt directives
- Follow website Terms of Service
- Use reasonable request rates
- Identify your scraper appropriately

---

## Key Features

| Feature | Description |
|---------|-------------|
| Keyless | No API keys required |
| robots.txt | Automatic compliance checking |
| Rate Limiting | Configurable request delays |
| User-Agent | Rotation and customization |
| Caching | Response caching support |
| Retries | Automatic retry with backoff |

---

## Quick Start

### 1. Configure Scraper

```typescript
const credentials = {
  baseUrl: 'https://example.com',
  userAgent: 'chrome_windows',
  requestDelay: 1000,
  respectRobotsTxt: true,
  followRedirects: true,
  retryOnFailure: true,
};
```

### 2. Create Executor

```typescript
import { KeylessScraperHandshakeExecutor } from './1.3.11.a_fileKeylessScraperHandshakeExecutor';

const executor = new KeylessScraperHandshakeExecutor();

// Initialize
const authResult = await executor.authenticate(credentials);
if (authResult.type === 'complete') {
  console.log('Scraper ready');
}
```

### 3. Scrape Pages

```typescript
// Scrape a page
const result = await executor.executeRequest({
  credentials,
  url: '/products',
  method: 'GET',
  headers: {},
});

if (result.success) {
  console.log('HTML length:', result.rawBody.length);
}
```

---

## Scraping Modes

### Polite (Recommended)

```typescript
const credentials = {
  baseUrl: 'https://example.com',
  requestDelay: 3000,  // 3 seconds
  respectRobotsTxt: true,
  rotateUserAgents: false,
};
// ~1,200 requests/hour
```

### Moderate

```typescript
const credentials = {
  baseUrl: 'https://example.com',
  requestDelay: 1000,  // 1 second
  respectRobotsTxt: true,
  rotateUserAgents: false,
};
// ~3,600 requests/hour
```

### Fast (Use Carefully)

```typescript
const credentials = {
  baseUrl: 'https://example.com',
  requestDelay: 500,  // 0.5 seconds
  respectRobotsTxt: true,
  rotateUserAgents: true,
};
// ~7,200 requests/hour
```

---

## User-Agent Options

| Option | Browser |
|--------|---------|
| `chrome_windows` | Chrome on Windows |
| `chrome_mac` | Chrome on macOS |
| `chrome_linux` | Chrome on Linux |
| `firefox_windows` | Firefox on Windows |
| `firefox_mac` | Firefox on macOS |
| `safari_mac` | Safari on macOS |
| `edge_windows` | Edge on Windows |
| `custom` | Your own string |

### User-Agent Rotation

```typescript
const credentials = {
  baseUrl: 'https://example.com',
  rotateUserAgents: true,  // Cycles through browser UAs
};
```

---

## DOM Parser

Use the DOM parser for extracting data:

```typescript
import { KeylessScraperDomParser } from './1.3.11.d_fileKeylessScraperDomParser';

const parser = new KeylessScraperDomParser(html, 'https://example.com');

// Get meta information
const meta = parser.getMetaInfo();
console.log('Title:', meta.title);
console.log('Description:', meta.description);

// Extract links
const links = parser.getLinks();
const internalLinks = parser.getInternalLinks();
const externalLinks = parser.getExternalLinks();

// Extract images
const images = parser.getImages();

// Extract tables
const tables = parser.getTables();

// Get text content
const text = parser.getTextContent();

// Query selector (basic)
const articles = parser.querySelectorAll('article.post');
```

### Quick Utilities

```typescript
import { DomUtils } from './1.3.11.d_fileKeylessScraperDomParser';

// Quick extraction without creating parser
const title = DomUtils.getTitle(html);
const links = DomUtils.getLinks(html, baseUrl);
const text = DomUtils.getText(html);
const images = DomUtils.getImages(html, baseUrl);
```

---

## Configuration Reference

### Required Fields

| Field | Type | Description |
|-------|------|-------------|
| `baseUrl` | URL | Target website URL |

### Optional Fields

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `userAgent` | Select | chrome_windows | Browser to emulate |
| `customUserAgent` | Text | - | Custom UA string |
| `rotateUserAgents` | Boolean | false | Rotate through UAs |
| `requestDelay` | Number | 1000 | Ms between requests |
| `respectRobotsTxt` | Boolean | true | Follow robots.txt |
| `followRedirects` | Boolean | true | Follow HTTP redirects |
| `maxRedirects` | Number | 5 | Max redirect hops |
| `timeout` | Number | 30000 | Request timeout (ms) |
| `proxyUrl` | URL | - | Proxy server URL |
| `customHeaders` | JSON | - | Additional headers |
| `acceptCookies` | Boolean | true | Store cookies |
| `cacheResponses` | Boolean | false | Enable caching |
| `cacheTtl` | Number | 300 | Cache TTL (seconds) |
| `retryOnFailure` | Boolean | true | Retry failed requests |
| `maxRetries` | Number | 3 | Max retry attempts |
| `retryDelay` | Number | 2000 | Ms between retries |

---

## Robots.txt Compliance

The scraper automatically checks robots.txt:

```typescript
// Automatically fetched during authenticate()
const authResult = await executor.authenticate(credentials);

// Check if URL is allowed
const isAllowed = executor.isUrlAllowed('/products', credentials);

// Manually fetch robots.txt
const robotsResult = await executor.fetchRobotsTxt('https://example.com');
if (robotsResult.rules) {
  console.log('Crawl delay:', robotsResult.rules.crawlDelay);
  console.log('Disallowed:', robotsResult.rules.disallowed);
  console.log('Sitemaps:', robotsResult.rules.sitemaps);
}
```

---

## Scraping Multiple Pages

```typescript
const urls = [
  '/page1',
  '/page2',
  '/page3',
];

// Sequential (respects rate limiting)
const results = await executor.scrapeMultiple(credentials, urls);

for (const [url, result] of results) {
  if (result.success) {
    console.log(`${url}: ${result.rawBody.length} bytes`);
  }
}
```

---

## Link Extraction

```typescript
// Extract links from response
const links = KeylessScraperHandshakeExecutor.extractLinks(
  result.rawBody,
  credentials.baseUrl
);

console.log('Found links:', links.length);
```

---

## Error Handling

```typescript
const result = await executor.executeRequest(context);

if (!result.success) {
  switch (result.statusCode) {
    case 403:
      if (result.errorCode === 'ROBOTS_BLOCKED') {
        console.error('URL blocked by robots.txt');
      } else {
        console.error('Access forbidden');
      }
      break;
    case 404:
      console.error('Page not found');
      break;
    case 429:
      console.error('Rate limited - slow down');
      break;
    case 500:
    case 502:
    case 503:
      console.error('Server error - retry later');
      break;
    default:
      console.error('Error:', result.error);
  }
}
```

---

## Caching

```typescript
const credentials = {
  baseUrl: 'https://example.com',
  cacheResponses: true,
  cacheTtl: 300,  // 5 minutes
};

// First request - fetches from server
const result1 = await executor.executeRequest(context);
// result1.headers['x-cache'] is undefined

// Second request (within TTL) - from cache
const result2 = await executor.executeRequest(context);
// result2.headers['x-cache'] === 'HIT'

// Clear cache manually
executor.clearCache();
```

---

## Proxy Support

```typescript
const credentials = {
  baseUrl: 'https://example.com',
  proxyUrl: 'http://proxy.example.com:8080',
};
```

---

## Best Practices

### Do ✅

- Respect robots.txt directives
- Use reasonable delays (≥1 second)
- Set appropriate timeouts
- Cache responses when possible
- Handle errors gracefully
- Identify your scraper appropriately

### Don't ❌

- Ignore robots.txt
- Hammer servers with rapid requests
- Scrape login-required content
- Circumvent access controls
- Store personal/private data
- Violate Terms of Service

---

## Rate Limit Guidelines

| Delay | Requests/Hour | Use Case |
|-------|---------------|----------|
| 5s | ~720 | Very polite, research |
| 3s | ~1,200 | Polite, most sites |
| 1s | ~3,600 | Moderate, lenient sites |
| 500ms | ~7,200 | Fast, permissive sites |
| <500ms | >7,200 | Not recommended |

---

## Troubleshooting

### "URL blocked by robots.txt"

The target URL is disallowed in robots.txt. Either:
1. Use a different URL
2. Set `respectRobotsTxt: false` (use responsibly)

### Rate Limited (429)

1. Increase `requestDelay`
2. Wait before retrying
3. Consider using a proxy

### Connection Timeouts

1. Increase `timeout` value
2. Check network connectivity
3. Try different user agent

### Empty Responses

1. Check if JavaScript rendering required
2. Verify URL is correct
3. Check for redirects

---

## Files in This Module

| File | Purpose |
|------|---------|
| `1.3.11.a_fileKeylessScraperHandshakeExecutor.ts` | Core scraper executor |
| `1.3.11.b_fileKeylessScraperCredentialFormFields.tsx` | React configuration UI |
| `1.3.11.c_fileKeylessScraperWhitepaperDocumentation.ts` | Technical specification |
| `1.3.11.d_fileKeylessScraperDomParser.ts` | HTML parsing utilities |
| `1.3.11.1.a_fileKeylessScraperProtocolReadme.md` | This documentation |

---

## Related Resources

- [Robots.txt Standard](https://www.robotstxt.org/)
- [Web Scraping Best Practices](https://en.wikipedia.org/wiki/Web_scraping)
- [HTTP Status Codes](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status)

---

## Version History

| Version | Date | Changes |
|---------|------|---------|
| 1.0.0 | 2024-12-03 | Initial release |

---

*Protocol OS - Configure once, connect anywhere.*
